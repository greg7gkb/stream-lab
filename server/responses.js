export const responses = [
  `Large language models work by predicting the next token in a sequence given all the previous tokens. During training, the model sees vast amounts of text and learns statistical patterns about which words tend to follow which other words in which contexts. This is done using the transformer architecture, which uses attention mechanisms to weigh the relevance of different parts of the input when producing each output token. The result is a model that can complete sentences, answer questions, write code, summarize documents, and much more — all by repeatedly predicting the next most-likely token. At inference time, the model samples from a probability distribution over the vocabulary at each step, which is why responses can vary even for identical prompts. Temperature controls the sharpness of this distribution: lower temperatures make the model more deterministic and focused, while higher temperatures introduce more randomness and creativity. Top-p sampling further refines this by only considering tokens whose cumulative probability exceeds a threshold, cutting off the long tail of unlikely options. Understanding these mechanics helps explain why LLMs can hallucinate — the model is always generating plausible-sounding continuations, even when it lacks the underlying knowledge to be accurate.`,

  `Streaming responses from a language model improves perceived performance significantly. Instead of waiting for the entire response to be generated before displaying anything, the client receives tokens as they are produced and renders them incrementally. This creates a typewriter effect that feels more responsive and alive. From a technical standpoint, streaming is typically implemented using Server-Sent Events or chunked HTTP transfer encoding. The server writes tokens to the response stream as they are produced, and the client reads them in real time using the Fetch API's ReadableStream interface or the native EventSource API. One subtlety is that EventSource only supports GET requests, so if you need to send a request body — for example, to pass the user's prompt — you typically use fetch with a POST request and handle the streaming manually by reading from response.body. The AbortController API lets you cancel an in-progress stream, which is useful for implementing a Stop button. Proper cleanup on both client and server is important to avoid resource leaks when streams are abandoned mid-flight.`,

  `React's state model pairs naturally with streaming UIs. As each token arrives, you append it to a piece of state, and React re-renders the component to show the updated text. Because React batches state updates, you want to be careful about how you accumulate tokens — using a functional updater like setState(prev => prev + token) ensures you always append to the latest value rather than a stale closure. For best performance, you can use a ref to accumulate tokens and only flush to state periodically, though for most LLM streaming use cases the token rate is low enough that per-token state updates work fine. The blinking cursor effect is a small CSS animation that gives users a visual cue that the stream is ongoing. It should disappear once the stream completes. You can track streaming state with a boolean flag in React state and conditionally render the cursor element. useEffect cleanup functions are important here: if the component unmounts while a stream is in progress, you need to abort the fetch to avoid state updates on an unmounted component, which would trigger React warnings.`,

  `The transformer architecture revolutionized natural language processing when it was introduced in the paper Attention Is All You Need in 2017. Prior to transformers, sequence models relied on recurrent neural networks, which processed tokens one at a time and struggled to maintain long-range dependencies. Transformers replaced recurrence with self-attention, a mechanism that allows every token to attend to every other token in the sequence simultaneously. This parallelism made transformers much faster to train on modern hardware and allowed them to scale to far larger datasets and model sizes than was previously feasible. The attention mechanism computes a weighted sum of value vectors, where the weights are determined by the compatibility between query and key vectors. Multi-head attention runs several attention operations in parallel, each learning to focus on different aspects of the input. Positional encodings are added to the input embeddings to give the model information about the order of tokens, since the attention operation itself is permutation-invariant. Stacking many transformer layers, each consisting of attention and feed-forward sublayers with residual connections and layer normalization, produces models capable of remarkable generalization.`,

  `Building a good chat interface requires thinking carefully about user experience details that are easy to overlook. The input field should submit on Enter and allow Shift+Enter for newlines, matching user expectations from other chat products. While a response is streaming, the Send button should be replaced by or supplemented with a Stop button, giving the user agency to interrupt a long response. The streaming text area should auto-scroll to keep the latest tokens visible, but should pause scrolling if the user manually scrolls up to read earlier content. Error states need to be handled gracefully — network failures, server errors, and timeouts should all produce clear feedback rather than silent failures. Accessibility matters too: streaming content should be announced to screen readers at an appropriate rate, not for every single token. The cursor animation should respect the prefers-reduced-motion media query. Empty and loading states should be distinct: an empty text area before the first message is different from a text area that is waiting for the first token of a response.`,
];

export function pickResponse(prompt) {
  if (!prompt || prompt.trim().length === 0) {
    return responses[0];
  }
  // Simple hash to pick a consistent response for a given prompt
  let hash = 0;
  for (const ch of prompt) {
    hash = (hash * 31 + ch.charCodeAt(0)) & 0xffffffff;
  }
  return responses[Math.abs(hash) % responses.length];
}
